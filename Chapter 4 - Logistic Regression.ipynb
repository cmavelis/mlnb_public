{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49036d26-2b7f-478c-8d75-c30564c3b17a",
   "metadata": {},
   "source": [
    "# Chapter 4 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dc3b01-fa4c-404f-8575-5b673746e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/jtooates/mlnb_public.git &> /dev/null\n",
    "    !mv mlnb_public/* . &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dec5259-b2f5-4164-8fd1-fbf80646d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Chapter\\ 4\\ Utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0e502-f9dd-4f37-b563-f0644d14cc0c",
   "metadata": {},
   "source": [
    "In chapter 2 we explored the Perceptron Learning Algorithm (PLA), which learns a weight vector ${\\bf w}$ such that ${\\tt sign}({\\bf w} \\cdot {\\bf x_i}) = y_i$ for all instances $({\\bf x_i}, y_i)$ in a linearly separable training set.  Recall that the output of the Perceptron for a new ${\\bf x}$ is either ${\\tt -1}$ or ${\\tt +1}.$  If the training set contains loan applications (i.e., the values of ${\\bf x_i}$) and whether the applicant paid off their loan on time (i.e., the values of $y_i$), the Perceptron will tell you if a new applicant will (${\\tt +1}$) or won't (${\\tt -1}$) pay off their loan on time.  If the training set contains symptoms and whether the patient had appendicitis, the Perceptron will tell tell you if the patient needs or does not need an appendectomy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3eadf-420e-423f-b865-50b63d5218f8",
   "metadata": {},
   "source": [
    "In this chapter we'll explore the Logistic Regression (LR) algorithm, which also learns weight vectors but is a probabilistic classifier.  Rather than producing a class label as output, LR produces probabilities of class labels, which is often very useful information.  To manage risk, a loan officer may approve larger loans for applicants that have a 99% chance of paying back their loans on time compared to those who have an 80% chance of paying back their loans on time.  Similarly, a doctor would not want to proceed with an appendectomy if the chance of needing one is only 70%, but would order more tests to get more certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed99ad-17f2-4424-9900-e3e554db8a9c",
   "metadata": {},
   "source": [
    "Logistic Regression is a useful algorithm in its own right, and learning about it will introduce a number of basic building blocks and ideas that we'll see in later chapters on neural networks.  The remainder of this chapter is organized as follows:\n",
    "* Section 4.1 describes the problem that LR solves and how it produces probabilities as opposed to class labels.\n",
    "* Section 4.2 is a quick review of a few key concepts from probability theory that are needed to derive the LR algorithm.\n",
    "* Section 4.3 applies the machinery of gradient descent from chapter 3 to derive the LR update rule, introducing the cross-entropy loss for probabilistic classifiers.  This section contains a lot of math related to deriving the gradient.  It is perfectly OK to read the first part that decribes the function to be optimized and the last part that shows final update rule and explains why it makes intuitive sense.\n",
    "* Section 4.4 contains an implementation of the algorithm, shows the behavior on a sample dataset, and identifies some practical issues that need attention when using LR on real data.\n",
    "* Section 4.5 explains how to adapt LR to deal with more than two classes.\n",
    "* Section 4.6 has a few closing thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762137aa-d229-4fad-ac88-144dde8b2b11",
   "metadata": {},
   "source": [
    "## 4.1 From Weight Vectors to Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3563f1e-037b-4cd4-a5ca-bd849df78f20",
   "metadata": {},
   "source": [
    "LR takes as input, just like the PLA, a training set with feature vectors ${\\bf x} \\in \\mathbb{R}^n$.  If a bias is needed, which is typically the case, we'll assume that one of the features is a constant value (e.g., $x_0 = 1$).  Like the Perceptron, LR is a binary classifier.  To make the math easier later on the class label will be either ${\\tt 0}$ or ${\\tt 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77c42-69cd-4d4a-a733-f1ba5a1d8229",
   "metadata": {},
   "source": [
    "Given a training set, LR learns a weight vector ${\\bf w} \\in \\mathbb{R}^n$ that is used to compute the probability that the correct class label is either ${\\tt 0}$ or ${\\tt 1}$.  Let $p^1_{\\bf w}({\\bf x})$ denote the probability that the true class label for input ${\\bf x}$ is ${\\tt 1}$.  Likewise, $p^0_{\\bf w}({\\bf x})$ denotes the probability that the true class label for input ${\\bf x}$ is ${\\tt 0}$.  Because they are probabilities, it must be the case that $0 \\leq  p^1_{\\bf w}({\\bf x}) \\leq 1$ and likewise for $p^0_{\\bf w}({\\bf x})$.  And because there are only two possible class labels it must be the case that $p^1_{\\bf w}({\\bf x}) + p^0_{\\bf w}({\\bf x}) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b97700-5ca6-4ae9-8ad8-78fca30e2b8d",
   "metadata": {},
   "source": [
    "Note that both $p^1_{\\bf w}$ and $p^0_{\\bf w}$ depend on ${\\bf w}$, the learned weight vector.  Different weight vectors will produce different probabilities for the same input ${\\bf x}$.  Two questions immediately come to mind.  First, how do weight and input vectors get turned into probabilities?  Second, what loss function does LR minimize in gradient descent to find a \"good\" weight vector?  We'll answer the first question now and provide intuition for the second, with a mathematically precise answer waiting until section 4.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c17639-0ecd-4665-8d20-7fb1c422f55f",
   "metadata": {},
   "source": [
    "Recall that the PLA computed the value of ${\\bf w} \\cdot {\\bf x}$ and handed that value to the ${\\tt sign}()$ function to determine the class label.  That function is shown below, with ${\\bf w} \\cdot {\\bf x}$ on the horizontal axis and $y$ on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edda0e6-9736-49cf-9022-256567b3138e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94482ef4-92c7-405c-a015-2a786cb93696",
   "metadata": {},
   "source": [
    "Note that the sign function is a step.  It doesn't matter if ${\\bf w} \\cdot {\\bf x}$ is 0.1 or 1 or 10.  As long as that value is positive, the instance is classified as positive.  Likewise, it doesn't matter if ${\\bf w} \\cdot {\\bf x}$ is -0.1 or -1 or -10.  As long as that value is negative, the instance is classified as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483798d-e800-4d31-8ed4-da20aa0788f7",
   "metadata": {},
   "source": [
    "Logistic Regression computes the value of ${\\bf w} \\cdot {\\bf x}$. But instead of passing it to the ${\\tt sign}$ function like the PLA, the *logistic* or *sigmoid* function is used, which is denoted $\\sigma$ and has the following form:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "We'll explain where this function comes from in a moment, but for now we'll explore some nice properties that it has as shown in the plot below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e9bd07-006b-4c87-8733-9623c4374fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_logistic(generic = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2ef84-16b6-4d6a-804c-f35c019a555e",
   "metadata": {},
   "source": [
    "The horizontal axis is ${\\bf w} \\cdot {\\bf x}$, just like in the previous plot.  But now the vertical axis is not the class label, it is the *probability* that the class label for instance ${\\bf x}$ is ${\\tt 1}$ given the current weight vector.  That is, the vertical axis is:\n",
    "    \n",
    "$$\n",
    "p^1_{\\bf w}({\\bf x}) = \\frac{1}{1 + e^{-{\\bf w} \\cdot {\\bf x}}}\n",
    "$$\n",
    "\n",
    "Gone is the step behavior of the ${\\tt sign()}$ function; $p^1_{\\bf w}({\\bf x})$ varies smoothly as a function of ${\\bf w} \\cdot {\\bf x}.$  The sigmoid function is sometimes called a \"squashing\" function because its domain is the entire real line (i.e., possible values of ${\\bf w} \\cdot {\\bf x}$) which is mapped or squashed into the range $[0, 1]$, allowing us to interpret its output as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994b445-950f-4d30-8cc4-8176b51e8ef3",
   "metadata": {},
   "source": [
    "Suppose ${\\bf w} \\cdot {\\bf x} = 0$.  Then it is the case that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p^1_{\\bf w}({\\bf x}) & = & 1 / (1 + e^{-{\\bf w} \\cdot {\\bf x}} ) \\\\\n",
    "& = & 1/(1 + e^0) \\\\\n",
    "& = & 1/(1 + 1) \\\\\n",
    "& = & 0.5\n",
    "\\end{eqnarray*}\n",
    "\n",
    "And because $p^1_{\\bf w}({\\bf x}) + p^0_{\\bf w}({\\bf x}) = 1$, it is also the case that $p^0_{\\bf w}({\\bf x}) = 0.5$.  That is, when ${\\bf w} \\cdot {\\bf x} = 0$ the instance is equally likely to belong to either class.  This is reminiscent of the Perceptron where points for which ${\\bf w} \\cdot {\\bf x} = 0$ were on the separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0f9a6-a53a-49aa-97f3-82232f412394",
   "metadata": {},
   "source": [
    "When ${\\bf w} \\cdot {\\bf x} > 0$ the value of $p^1_{\\bf w}({\\bf x}) > 0.5$ as is clear from the plot above, and it is more likely that the correct class label is ${\\tt 1}$ as opposed to ${\\tt 0}$.  In contrast to the Perceptron, which doesn't care about the magnitude of ${\\bf w} \\cdot {\\bf x}$, for LR the larger the value of ${\\bf w} \\cdot {\\bf x}$ the more certain the classifier becomes.  When ${\\bf w} \\cdot {\\bf x} = 0.1$, $p^1_{\\bf w}({\\bf x}) = 0.52$.  That probability becomes $0.73$ when ${\\bf w} \\cdot {\\bf x} = 1$ and $0.99$ when ${\\bf w} \\cdot {\\bf x} = 10$.  The larger the value of ${\\bf w} \\cdot {\\bf x}$ the higher the probability that the instance belongs to class ${\\tt 1}$ and the lower the probability that the instance belongs to class ${\\tt 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22814ed-363a-4fb5-974b-d0a6508c6188",
   "metadata": {},
   "source": [
    "When ${\\bf w} \\cdot {\\bf x} < 0$ the value of $p^1_{\\bf w}({\\bf x}) < 0.5$ as is clear from the plot above, and it is more likely that the correct class label is ${\\tt 0}$ as opposed to ${\\tt 1}$.  When ${\\bf w} \\cdot {\\bf x} = -0.1$, $p^1_{\\bf w}({\\bf x}) = 0.48$.  That probability becomes $0.27$ when ${\\bf w} \\cdot {\\bf x} = -1$ and $0.01$ when ${\\bf w} \\cdot {\\bf x} = -10$.  The more negative the value of ${\\bf w} \\cdot {\\bf x}$ the higher the probability that the instance belongs to class ${\\tt 0}$ and the lower the probability that the instance belongs to class ${\\tt 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3842846-dd87-4d99-a1ef-17fb92dc907e",
   "metadata": {},
   "source": [
    "But where did the sigmoid function come from?  Surely there are other functions that squash the real line into the range $[0, 1]$ that would work just as well.  It turns out that the sigmoid is the only function for which the log odds are linear.  Let's unpack that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fbd8c-c7f5-4073-85c8-0295e5d67af7",
   "metadata": {},
   "source": [
    "Recall that the *odds* of something happening is the ratio of the probability of that thing happening to the probability of it not happening.  If $p$ is the probability of your team winning the game, then the odds of them winning is just $p/(1-p)$.  When they have a 50-50 chance, the odds are 1.  If they are more likely to win than lose then the odds are greater than one, and if they are more likely to lose than win the odds are less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200ba4d-7a1c-4cee-a2ea-044f16aff0bd",
   "metadata": {},
   "source": [
    "What does it mean for the log odds to be linear in the context of LR?  Mathematically it means the following:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\frac{p^1_{\\bf w}({\\bf x})}{p^0_{\\bf w}({\\bf x})} \\right) =  {\\bf w} \\cdot {\\bf x}\n",
    "$$\n",
    "\n",
    "In words, the log of the odds of class ${\\tt 1}$ being the correct class is a linear function of the instance ${\\bf x}$.  A unit change in one of the feature values ($x_i$) leads to a constant change in the log odds where the magnitude of that change is controlled by the magnitude of the corresponding weight ($w_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ea66d-e8a0-40b6-81f8-6ad251cb24cf",
   "metadata": {},
   "source": [
    "As shown below, requiring the log odds to be a linear function of ${\\bf x}$ leads directly to the sigmoid as the representation for $p^1_{\\bf w}({\\bf x})$, which is abbreviated as $p^1_{\\bf w}$ to keep things concise.  The derivation starts with the requirement of linear log odds, and then exponentiates both sides to get rid of the log.  From there it's just algebra, including the last step where the top and bottom of the right-hand side are multiplied by $1/e^{{\\bf w} \\cdot {\\bf x}}$ to get to the form of the sigmoid introduced earlier in this chapter.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ln(p^1_{\\bf w}/p^0_{\\bf w}) & = & {\\bf w} \\cdot {\\bf x} \n",
    "\\tag*{Linear log odds}\n",
    "\\\\[0.2cm]\n",
    "p^1_{\\bf w}/p^0_{\\bf w} & = & e^{{\\bf w} \\cdot {\\bf x}} \n",
    "\\tag*{Exponentiate both sides} \n",
    "\\\\[0.2cm]\n",
    "p^1_{\\bf w}/(1 - p^1_{\\bf w}) & = & e^{{\\bf w} \\cdot {\\bf x}} \n",
    "\\tag*{Rewrite $p^0_{\\bf w}$ in terms of $p^1_{\\bf w}$}\n",
    "\\\\[0.2cm]\n",
    "p^1_{\\bf w} & = & (1 - p^1_{\\bf w}) e^{{\\bf w} \\cdot {\\bf x}} \n",
    "\\tag*{Solve for $p^1_{\\bf w}$}\n",
    "\\\\[0.2cm]\n",
    "p^1_{\\bf w} & = & e^{{\\bf w} \\cdot {\\bf x}} - p^1_{\\bf w} e^{{\\bf w} \\cdot {\\bf x}} \\\\[0.2cm]\n",
    "p^1_{\\bf w} + p^1_{\\bf w} e^{{\\bf w} \\cdot {\\bf x}} & = & e^{{\\bf w} \\cdot {\\bf x}} \\\\[0.2cm]\n",
    "p^1_{\\bf w}(1 + e^{{\\bf w} \\cdot {\\bf x}}) & = & e^{{\\bf w} \\cdot {\\bf x}} \\\\[0.2cm]\n",
    "p^1_{\\bf w} & = & \\frac{e^{{\\bf w} \\cdot {\\bf x}}}{1 + e^{{\\bf w} \\cdot {\\bf x}}}\n",
    "\\tag*{Divide top and bottom by $e^{{\\bf w} \\cdot {\\bf x}}$}\n",
    "\\\\[0.2cm]\n",
    "p^1_{\\bf w} & = & \\frac{1}{1 + e^{-{\\bf w} \\cdot {\\bf x}}} \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e5f41-37c4-48e0-a0bd-7d97e14b4f81",
   "metadata": {},
   "source": [
    "So where are we now?  We know that Logistic Regression is a probabilistic binary classifer.  Given a training set it learns a weight vector, and that weight vector is combined with new instances via the sigmoid function to get the probability that they belong to class ${\\tt 1}$ or ${\\tt 0}$.  We use the sigmoid because it makes the log odds of class ${\\tt 1}$ linear in the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abda24-b376-4c0e-a38b-a61f0584bda9",
   "metadata": {},
   "source": [
    "But how does LR arrive at a weight vector?  What makes one weight vector better than another?  Intuitively, LR tries to maximize the probability of the correct class label.  If $({\\bf x_i}, y_i = 1)$ is a training instance, the algorithm will adjust the weight vector to increase $p^1_{\\bf w}({\\bf x_i})$ and thus decrease $p^0_{\\bf w}({\\bf x_i})$.  If $({\\bf x_i}, y_i = 0)$ is a training instance, the weight vector will change to increase $p^0_{\\bf w}({\\bf x_i})$ and thus decrease $p^1_{\\bf w}({\\bf x_i})$. The final weight vector will reflect the push and pull of all of the instances in the training set.  This is an example of maximum likelihood estimation, which is covered in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a03194-c431-4801-bf00-7d8982e80ef8",
   "metadata": {},
   "source": [
    "We'll make the intuition above formal in section 4.3, right after a very quick review of a few concepts from probability needed to derive the LR update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bffc03-a880-4d53-b492-2ae93d33d828",
   "metadata": {},
   "source": [
    "## 4.2 Just Enough Probability to Derive the Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dface12-c816-41af-b77d-e5cfe47a8393",
   "metadata": {},
   "source": [
    "Because Logistic Regression is a probabilistic classifier, we'll use a few simple concepts from probability theory during the derivation of it's update rule.  This section contains a very quick refresher for those concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed852d08-d4f6-4629-b8fb-87bcc37e0ce4",
   "metadata": {},
   "source": [
    "### Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c884b2-d865-48d5-b04c-ad25e752c25e",
   "metadata": {},
   "source": [
    "A **random variable** is a variable whose value is the result of some process or event that involves uncertainty.  A **discrete random variable** is one that can take on a finite number of distinct values, like ${\\tt heads}$ or ${\\tt tails}$ representing the outcome of a coin flip or numbers in $\\{1, 2, 3, 4, 5, 6\\}$ representing the outcome of a die roll.  A **continuous random variable** is one that can take on an infinite number of distinct values, typically subsets of the real line, like daily rainfall amounts or closing prices of financial securities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00961aa-30de-4509-9856-24799e2230c0",
   "metadata": {},
   "source": [
    "For discrete random variable $X$, let $p(X = x)$ denote the probability that $X$ takes on value $x$.  It must be the case that $0 \\leq p(X = x) \\leq 1$ for all $x$ and that the sum of $p(X = x)$ over all $x$ is 1.  If $X$ represents the outcome of a fair coin flip then $p(X = {\\tt heads}) = p(X = {\\tt tails}) = 0.5$ and $p(X = {\\tt heads}) + p(X = {\\tt tails}) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eac410-819e-40a6-a5ad-9316bb89f1e7",
   "metadata": {},
   "source": [
    "A **joint probability** is the probability of two or more random variables taking on specific values at the same time.  If $X$ represents the outcome of a coin flip and $Y$ represents the outcome of a die roll, then $p(X = {\\tt heads}, Y = {\\tt 5})$ is the probability that the coin comes up ${\\tt heads}$ and the die comes up ${\\tt 5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e982416-a223-4a46-a60e-b109d4ac536e",
   "metadata": {},
   "source": [
    "Two random variables are **independent** if knowing the value of one gives you no information about the value of the other.  Suppose I ask you what the probability is that a fair die will come up ${\\tt 5}$.  You would say $1/6$.  If I tell you that the coin came up ${\\tt heads}$ and ask the same question, you'll still say $1/6$ because the coin flip and the die roll do not influence each other, they are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90444aa9-7e6e-498d-9ad9-73859bc626f6",
   "metadata": {},
   "source": [
    "Now suppose I ask you what the probability is that a person chosen at random in the US is above the median age.  You'd clearly answer 0.5 because, by definition, half of the people are older than the median and half are younger.  But if I tell you that the person is retired your estimate of the probability that they are above the median age will go up, perhaps significantly.  That's because age and retirement status are dependent.  The younger you are the less likely you are to be retired and the older you are the more likely you are to be retired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f88cc-050f-4a32-9bf7-17ecee5391cd",
   "metadata": {},
   "source": [
    "More formally, two random variables are independent if and only if the joint probability is equal to the product of the individual probabilities:\n",
    "\n",
    "$$p(X = x, Y = y) = p(X = x) p(Y = y)$$\n",
    "\n",
    "If you want to know $p(X = {\\tt heads}, Y = {\\tt 5})$ it suffices to compute $p(X = {\\tt heads}) p(Y = {\\tt 5})$ because coin flips are die rolls are independent.  But it is *not* the case that $p(X = {\\tt above\\ median\\ age}, Y = {\\tt retired})$ is equal to $p(X = {\\tt above\\ median\\ age}) p(Y = {\\tt retired})$ because age and retirement status are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d390d-83e1-4e56-b069-35216e5d9ec9",
   "metadata": {},
   "source": [
    "In the two variable case, a **conditional probability** is the probability that one of the variables takes on a specific value given the value of the other and is written $p(X = x | Y = y)$.  If you're in a world where you've observed the value of $Y$ to be $y$, what is the probability that $X = x$?  For example, $p(X = {\\tt above\\ median\\ age} | Y = {\\tt retired})$ is the probability that a person who is known to be retired is above the median age. That is the fraction of retired people who are above the median age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f6d5b-30f8-4fac-8ddb-dd2494c0161d",
   "metadata": {},
   "source": [
    "The conditional and joint probabilities are related by the following equality known as the **chain rule of probability**:\n",
    "\n",
    "$$p(X = x, Y = y) = p(X = x | Y = y) p(Y = y)$$\n",
    "\n",
    "If $X$ and $Y$ are independent, then $p(X = x | Y = y) = p(X = x)$.  That's another way of saying that knowing the value of $Y$ does not change your mind about the probability of $x$ being the value of $X$.  We show that mathematically below, where the second line follows from the first due to idendendence of $X$ and $Y$, and the third line follows from the second after dividing both sides by $p(Y = y)$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p(X = x, Y = y) & = & p(X = x | Y = y) p(Y = y) \\\\[0.2cm]\n",
    "p(X = x) p(Y = y) & = & p(X = x | Y = y) p(Y = y) \\\\[0.2cm]\n",
    "p(X = x) & = & p(X = x | Y = y)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ec663-6c1b-4dad-aaf5-74f9e30ea172",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d0228-a234-469a-8a73-1a6d53d3039c",
   "metadata": {},
   "source": [
    "Suppose you have a biased coin that comes up heads with unknown probability $\\theta$.  You flip the coin 10 times and get 7 heads and 3 tails.  What value would you assign to $\\theta$?  The obvious answer is $7/10$, the number of heads divided by the number of flips.  It turns out that this obvious answer is the **maximum likelihood estimate** (MLE) of $\\theta$.  That is, it is the value that maximizes the probability of the data that you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80318b49-fa16-48ed-83d0-819fd8558f02",
   "metadata": {},
   "source": [
    "Suppose the sequence of coin flips was $D = \\{{\\tt H, H, T, H, H, H, H, T, T, H}\\}$.  Let $L(D; \\theta)$ denote the likelihood of the data, the probability of observing $D$ given a specific value of $\\theta$, and let $p_\\theta()$ denote a probability given a specific value of $\\theta$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "L(D; \\theta) = p_\\theta({\\tt H, H, T, H, H, H, H, T, T, H})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Because the coin flips are independent of one another, we can rewrite the joint probability as a product of individual probabilities and simplify:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "L(D; \\theta) & = & p_\\theta({\\tt H}) p_\\theta({\\tt H}) p_\\theta({\\tt T}) p_\\theta({\\tt H}) p_\\theta({\\tt H}) p_\\theta({\\tt H}) p_\\theta({\\tt H}) p_\\theta({\\tt T}) p_\\theta({\\tt T}) p_\\theta({\\tt H}) \\\\\n",
    "& = & p_\\theta({\\tt H})^7 p_\\theta({\\tt T})^3 \\\\\n",
    "& = & p_\\theta({\\tt H})^7 (1 - p_\\theta({\\tt H}))^3 \\\\\n",
    "& = & \\theta^7 (1 - \\theta)^3\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The MLE for $\\theta$ is the value that maximizes $L(D; \\theta) = \\theta^7 (1 - \\theta)^3$, i.e., the probability of seeing 7 heads and 3 tails in 10 flips.  The plot below shows $L(D; \\theta)$ on the vertical axis as a function of $\\theta$ on the horizontal axis.  Note that the maximum lies at $\\theta = 0.7$, which accords with intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b799bf-4fae-46a8-a834-4506acac4673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_MLE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea88a29-9849-4795-9fd9-6ac08ea785e9",
   "metadata": {},
   "source": [
    "We're finally ready to derive the update rule for Logistic Regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb8672-e271-41de-8322-aef26f2dc69e",
   "metadata": {},
   "source": [
    "## 4.3 Deriving the Logistic Regression Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe7e7c-4bb0-4e8b-997b-afc612fd8d24",
   "metadata": {},
   "source": [
    "As described in Chapter 3, gradient descent is machine learning's hammer and it's looking for nails.  In this section we frame Logistic Regression in those terms which, as by now you know, involves defining a suitable loss function and computing the partial derivatives of that loss function with respect to the learnable parameters, i.e., the weights.  We'll take a slightly different approach here by first deriving the likelihood function and then computing the partial derivatives of that function with respect to the weights so that it can be **maximized** with **gradient ascent** (i.e., by adding the gradient to update the parameters as opposed to subtracting it).  But it's trivial to use standard gradient descent by treating the negative of the likelihood function as a loss to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1caabda-f007-47a0-9705-c78eba05d39e",
   "metadata": {},
   "source": [
    "This is a long section with lots of math.  If you want to skip to the end for the punchline, which is the update rule that we'll use for implementing Logistic Regression in section 4.4, that's OK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd12755-fa92-4af6-aa2b-d74782f63379",
   "metadata": {},
   "source": [
    "### Deriving the Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555d044-b020-4cf5-a27c-debe278d83ea",
   "metadata": {},
   "source": [
    "As usual, we'll assume a training set of $m$ instances $D = \\{({\\bf x_1}, y_1), \\ldots, ({\\bf x_m}, y_m))$ with ${\\bf x_i} \\in \\mathbb{R}^n$.  For LR, as noted above, $y_i \\in \\{0, 1\\}$.  Our goal is to find the **maximum likelihood** weight vector ${\\bf w} \\in \\mathbb{R}^n$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\underset{{\\bf w}}{\\arg\\max} \\ L(D; {\\bf w}) & = & \\underset{{\\bf w}}{\\arg\\max} \\ p_{\\bf w}(D) \n",
    "\\tag*{Definition of MLE} \\\\\n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\ p_{\\bf w}(({\\bf x_1}, y_1), \\ldots, ({\\bf x_m}, y_m)) \n",
    "\\tag*{Expand $D$}\\\\\n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\ \\prod_{i} p_{\\bf w}({\\bf x_i}, y_i) \n",
    "\\tag*{Independence} \\\\\n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\ \\ln \\left( \\prod_{i} p_{\\bf w}({\\bf x_i}, y_i) \\right)\n",
    "\\tag*{Log is monotonic} \\\\\n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\ \\sum_{i} \\ln \\left( p_{\\bf w}({\\bf x_i}, y_i) \\right)\n",
    "\\tag*{Log of product}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9c57c-188d-43c8-86ad-d94469761b68",
   "metadata": {},
   "source": [
    "We're looking for the weight vector that makes the observed data as likely as possible where that data consists of $m$ training instances.  Because the instances are independent the joint probability of the entire dataset can be rewritten as a product of probabilities of the individual instances.  For example, the fact that someone showed up at the bank yesterday applying for a loan that they will not pay off on time has no causal impact on my financial status (i.e., my ${\\bf x}$) or whether I'll pay my loan off on time (i.e., my $y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1c7be-c7c0-44d8-9385-7dfacd85aea0",
   "metadata": {},
   "source": [
    "This is all leading up to computing derivatives with respect to weights, and derivatives of products are painful.  Taking the log makes that problem go away because the log of a product becomes the sum of the logs of the individual terms.  This new expression is called the **log likelihood**.  Note that the log is always applied to probabilities, numbers between 0 and 1.  The plot below shows what the log looks like in that region applied to some probability $p$.  The log is monotone, as $p$ increases so does $\\ln(p)$, so the ${\\tt argmax}$ applied to a raw probability or the log of that probability will return the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199e1d62-1a80-410b-942b-772a956078d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEYCAYAAABcGYHrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAokklEQVR4nO3deXxcdb3/8dcnTdMla5ukSZs2TdN9o6Uru1gKBVRAxcvmVRFBfl4RrwsobvhT/HnVK1e9epEriLjQy3KrIqiACAi0QIG2lO5Nl3TJ3mZtmu37++Oc1GnMMmmSObO8n49HHpmZc+bM5ztz5rznfM9mzjlERESSgi5ARESigwJBREQABYKIiPgUCCIiAigQRETEp0AQERFAgRAWM7vTzH41SNN6wMy+ORjT6jLdO8zsZ4M9XfGY2blmtr2X4UVm5swsOZJ1xSozKzSzBjMbFnQt8ncKBMCfMTv/OszsWMj964KuLxzOuW855z4WdB2nyl+YTgu6jp445/7mnJvZed/M9prZyiBrGkrh/HAZyGfmnNvvnEtzzrX703rOzGJ2/h0Kg/lDNFwKBMCfMdOcc2nAfuA9IY/9Ouj6JP7pl3JkaA2uD845/YX8AXuBlV0euxN4GHgQqAfeBpaEDJ8APAZUAnuAT/Uy/QeAb4bcvxHYBdQAvwcmhAy7CNgO1AI/AZ4HPtbDdO8EfuXfHgn8CqgGjgKvAXm9tPfzwCagEbgPyAP+6Lf1GWBMyPiX+e0/CjwHzO4yrc/506oF/gcY2VdbgRcA579+A3BVGO+NA24GdgJHgB8DFjL8o8BWf9ifgck9tP8XwGf92wX+dD/h35/mv7YB5wMH/Md/CXQAx/x6bwOK/Od+GO9HRRXwpT7mg/8CnvTbvbK3+QhYBqwH6oBy4Pv+452vexNwCDjc2R5/eBLwBWC3Pz88DIwNGX4O8LL/eZYCH/Gn1Qq0+O17vJv6+/2ZdXl+Z93JwF1AO9DsT+s//ff8bqACb17aBMzrYVrX+591PVACfDxk2PnAAeB2oMz/7Eb5n/sR/3m3dX62IfPWtO6+syHTu82v7TBwBXApsMNv9x3hvP+9zTPAxf773+q/JxsjsvyLxIvE0h89B0Kz/6EPA/4fsC7kA38d+CqQAhT7M+WqHqYfOnOt8GeCRcAI4EfAC/6wHLwv//v8L82t/swRTiB8HHgcGO3XuxjI6KW96/BCoMCfyd8ATvdrehb4mj/uDLwFwIXAcP9LsQtICZnWq3gLtrH+l+3mvtraw5cwnPH/AGQBhXgL0Yv9YVf4dc3237svAy/30P6P4i/wgGvxvrj/EzLsd/7t8zl5oXHSfMLfv9z/jbfAWQAcJyQwu5kPaoGz8eah0fQyHwFrgX/2b6cBZ3R53YeAVGC+/16s9Id/2v98J/rv40+Bh/xhhXgL0Wv8zzMbWNh1Pu3lu9Kvz6zLczvrTvbvP0fIvA2s8t+PLLxwmA2M72Fa7wKm+uO9A2gCFoV8bm3Av/k1jQK+jffjaoz/vmyif4HQ5n9Ow/ECsBL4DZAOzMVbVhSH8f53vgfdzjOEfKcjtvyL5IvFwh89B8IzIffnAMf828uB/V3G/yLw8x6mHzpz3Qd8J2RYGt5Cvwj4ELA2ZJjh/YILJxA+iver77Qw23tdyP3HgP8KuX8L8Fv/9leAh0OGJQEHgfNDpvXBkOHfAe7pq63+/a5fwnDGPydk+MPAF/zbfwRu6FJnE92sJeAtSI7649yDF6adawK/AD7j3z6f8AJhYshjrwJX9zIfPBhyv9f5CO8X+deBnC7jdL7urC7v+33+7a3ABSHDxvvvY7I//TV9zae9zDv9+sx6qLunQFiB94v7DCCpn9/h3wK3hnxuLZy8pnrSDzbgY/QvEI4Bw/z76f74y0PGfx24Ioz3v9d5hgACQdsQwlcWcrsJGOn3R04GJpjZ0c4/4A68X9x9mQDs67zjnGvAW60s8IeVhgxzeKuq4fglXjfJajM7ZGbfMbPhvYxfHnL7WDf303qot8OvsSBk/K7vU0/PDW1rd8IZv6fXmgz8IOTz6Oz2+YfXcs7txlslXwici7fWccjMZuL92ny+h/p60lNN3SkNud3XfHQD3hraNjN7zcze3cu09uG9f53TXRMyza143TN5wCS8NaLB0t/PuEfOuWfxuo5+DJSb2b1mltHduGZ2iZmtM7Mav42X4q1hd6p0zjV3qTP0/Qq9HY5q528Mx/t+QM/fmd7e/079mWeGlAJh4EqBPc65rJC/dOfcpWE89xDeDAOAmaXirbYfxOubnBgyzELv98Y51+qc+7pzbg5wFvBuvDWOgepar+EtVA6ewnND2zoY44cqxetHDv1MRjnnXu5h/OeBK/G6vg769z+E16WwoYfnuDDq6EvoNHqdj5xzO51z1wDj8Lo/HvXfk06TQm4X4r1/ndO9pMt0R/rtLMVbQxqs9g3kM/uH13PO/dA5txivG2YG3rauk5jZCLy12u/hbSfLwtsuY71M+6TvFie/d+AtlEeH3M8Po/6e9Pb+92Uw5rF+USAM3KtAnZndbmajzGyYmc0zs6VhPPc3wPVmttCfsb8FvOKc2ws8Acw3syv8NZF/IcwZ08zeaWbz/T1X6vBWUdv7eFo4HgbeZWYX+Gscn8Xr8+xpQRuqt7aC9wuruB/j9+Ye4ItmNhfAzDLN7AO9jP888Em8bhnwui9uAV4M+SXYVdd6B6rX+cjMPmhmuf5a2VH/OaG1fcXMRvttvh5vgz5478VdZjbZn06umV3uD/s1sNLM/snMks0s28wW9qN9g/mZnTQtM1tqZsv9+awRr1++u88iBa9vvhJoM7NL8HbG6M3DePPHGDMrwPvsQ20ArvU/g4vx1hRPVW/vf1/KgSIzi9hyWoEwQP4C4z14XQ578Daq/QzIDOO5f8Hrl38M71fLVOBqf1gV8AG8/uBqvO0W6/EWwH3JBx7FC4OteAu8Ae/P7JzbDnwQb2NhFV673+OcawnjuT221Xcn8At/1fqfwhi/t9dag/crerWZ1QGbgUt6ecrzeH3BnYHwIt4vxBd6fIa3Y8GX/Xo/F05dfdTc13x0MfC2mTUAP8DrZw7tBnkeb0P6X4DvOeee8h//Ad7ePk+ZWT3eBs7l/mvux+te+Sxet9oGvA2b4G0PmOO377c9lH0ng/SZ+XVeaWZHzOyHQAbextYjeN1Q1XhrASdxztUDn8JbyB/B2zHg93281v/F637dg7cX3aOc/L26Fe+zOApch7dN4lT1+P6H4RH/f7WZvTGAGsJm/sYLiXL+r4QDeBuA/xp0PRIdzKwIb8E23DnXFnA5McnM/g9ewA5kTSAuaA0hipnZKjPL8le/78DrF10XcFkiMc3MxpvZ2WaW5O888FlgTdB1RQMdtRfdzsTrl00BtuDtynas96eISB9S8I4HmILXLbQa78DPhKcuIxERAdRlJCIivpjuMsrJyXFFRUVBlyEiElNef/31KudcbtfHYzoQioqKWL9+fdBliIjEFDPb193j6jISERFAgSAiIj4FgoiIAAoEERHxKRBERASIskAws4vNbLuZ7TKzLwRdj4hIIomaQPBP1fxjvLNSzgGuMbM5wVYlIpI4ouk4hGXALudcCYCZrQYuxzuHj4hIwnLOUdXQwr7qRvZWN7G/upEPLJnEpLGj+35yP0RTIBRw8qXsDtDNecPN7CbgJoDCwsLIVCYiMsQ6F/p7qxvZU9XoLfyrmthb3ci+6iYajv/97OZJBqcXjonrQLBuHuvusnr3AvcCLFmyRGfmE5GYUtvUSklVA3uqGtlb1cie6ib2VDWwt+rkhX5ykjFxzCiKclJZWjSWydmjKcpJpSg7lYKsUaQkD36PfzQFwgFOvrbpRP5+XVgRkZhxvK2dfdVNlFQ2egv/Su9Xf0lVIzWNf7/AYJLBxDHegn5x4RhvgZ+TypTsVArGjGL4sMhu5o2mQHgNmG5mU/Auyn013uXwRESijnOOyvrj7K5sZHdlw4mFf0llIweONNER0n8xLn0EU3JSuWhOHsW5qUzJSWNKzmgmjR3NiORhwTWii6gJBOdcm5l9EvgzMAy43zn3dsBliUiCa23vYF91E7srG9hV0cDuygZ2VzZSUtFAfUgXz6jhw5iSk8qCSVlccXoBU3NTKc5JoyhnNOkjhwfYgvBFTSAAOOeeBJ4Mug4RSTzHWtpPLPR3VtSzq8K7va+6ibaQn/v5GSOZOi6VK04vYNq4NIpzUynOTWN8xkiSkrrbFBo7oioQRESGWsPxNnZVNLCj3Fvo7yyvZ2dFAwePHqPzApLDkozJ2aOZlpvGRXPzmZabxrRxaUwdl0baiPhdbMZvy0QkoTW1tLGz3Fvw7/QDYGe5t+DvlJKcRHFOKqcXjuEDiycxPS+N6ePSmJydOiR78UQ7BYKIxLSWtg5KqhrYXlbP9rJ6dpTXs728ntKakxf8U3PTWFI0hmvGTWJ6Xjoz8tIpHDuaYTHezTOYFAgiEhOccxw8eoztZfVs8/+2l9VRUtl4oo8/Ockozk3ltIlZXLloEjPz05mR5/3i14K/bwoEEYk6jcfb/IV+HdsO//1/6F49BVmjmJWfzgWz85iVn87M/HSKc9ISsqtnsCgQRCQwzjkO1Taz5VAdWw/Xef/L6thX3XRinPQRycwan87lp09gVn4Gs8enMz0vnYwY2ZUzligQRCQi2to72F3ZyNuHann7kLfw33K4jtpjrSfGKcoezZzxGbx/0URm5acze3wGE8eMwkzdPZGgQBCRQdfc2s72sno2H6pl88E6thyqZWtZPS1tHQCMSE5iVn46l87PZ874DOZMyGBmfkZc79IZC/Tui8iAHGtpZ8vhOjYfrOWtg7VsPljLzooG2v0NvRkjk5lXkMmHz5zM3AmZzJmQQXFOKskRPk+P9E2BICJha25tZ1tZPW8dOMqmA14AhC78s1NTmFeQycrZecwryGDuhEx1+cQQBYKIdKutvYOdFQ1sOnCUjQdq2XTgKNsO15/YxTM7NYX5EzO5cE4e8woymV+QyfjMkVr4xzAFgoic2Md/Y2ktG0qPsLHU+/V/rLUdgPSRyZw2MZMbzytmwcRMTpuYpYV/HFIgiCSgxuNtbDxwlA2lR3lzv/dX1XAc8I7qnTM+g6uWTmLhpCxOm5hJUXZqzJ+4TfqmQBCJc8459lY38fq+I7yx/whv7j/K9rK6E+frn5KTynnTc1hYmMXCSVnMys/QwV0JSoEgEmeaW9vZdKCW1/cdORECnVfpSh+RzMLCLC5cMZ3TC7NYODGLMakpAVcs0UKBIBLjahpbWL+3hvX7jvDa3ho2H6yltd37+V+cm8qKWeNYPHkMiyePYVpumrp+pEcKBJEYc+BIE6/uqeG1vTW8uqeG3ZWNAKQMS+K0iZl89JwpLJk8lsWTxzBWv/6lHxQIIlHMOceeqkZe2eMt/F/dU3PifP7pI5NZMnkM7188kWVFY5lXkMnI4dFzfV6JPQoEkSjSGQDrSmpYV1LNupJqKuq9vX9y0kawfMpYbjx3CsumZDMzP12ndJZBpUAQCZBzjtKaY6wtqeLl3dWs3f33AMjLGMEZxdmcUZzN8uKxFOekar9/GVIKBJEIq6w/zsu7q3hpVxUv7ao+0QWUkzaCM6dmc2ZxNmdOzaYoe7QCQCJKgSAyxJpa2nhlTw0v7vRCYFtZPeCd9O3MqdncdF4xZ0/LZmpumgJAAqVAEBlkHR2OLYfreGFnJX/bUcXr+47Q0t5BSnISS4vGcNvFMzl7ag7zCjK1DUCiigJBZBBUNxznbzureH5HJX/bWUlVg3cg2Kz8dD5ydhHnTs9hadFY7QUkUU2BIHIKOjocmw7W8tdtFTy3vYJNB2txDsampnDu9BzOm57LudNzGJcxMuhSRcKmQBAJU11zKy/sqOTZbRU8v72S6sYWzGDhpCw+fcEMzp+Zy/yCTB0JLDFLgSDSi33VjTy9pZy/bK3gtb01tHU4skYP5x0zclkxaxznTs/V0cASNxQIIiE6OhwbDhzl6S3lPLOlnJ0VDQDMyEvjY+cWs3L2OE4vHKONwRKXFAiS8FraOlhbUs1Tb5fx9JZyKuqPMyzJWD5lLNcsK2Tl7DwKs0cHXabIkFMgSEJqbm3n+R2V/GlzGc9sLae+uY3RKcM4f2YuF87JY8XMPDJHDw+6TJGIUiBIwmhqaeO57ZU8+dZhnt1WQVNLO5mjhrNqbj4Xz83nnOk52i1UEpoCQeJac2s7z22v4PFNh3l2awXHWtvJSUvhvacXcMm88SwvHsvwYbo6mAgoECQOtbR18OKuSh7feJin3i6jsaWd7NQU3r+4gEvnj2f5lGxtFBbphgJB4kJHh2P9viP8bsNBnnjrMEebWskcNZz3LJjAu0+bwBnFY0nWmoBIr6IiEMzsA8CdwGxgmXNufbAVSazYXdnAmjcOsubNgxw8eoyRw5O4cE4+ly+YwHkzcnWxeJF+iIpAADYD7wN+GnQhEv2ONrXw+MZDPPrGQTaWHiXJ4JzpuXxu1QwumpNP6ohoma1FYktUfHOcc1sBnfpXetTe4XhhZyWPrC/lmS0VtLR3MCs/nS+/azaXLZigcwaJDIKoCIT+MLObgJsACgsLA65Ghtr+6iYeXl/Ko68foKyumbGpKVx3RiFXLp7I3AmZQZcnElciFghm9gyQ382gLznnfhfudJxz9wL3AixZssQNUnkSRVraOnhqSxmrXy3lxV1VJBm8Y0Yud142hxWz8rRdQGSIRCwQnHMrI/VaEptKa5p46NX9PLy+lKqGFgqyRvGvK2fwT0snMj5zVNDlicS9mOsykvjS0eF4fkclD67dy3M7KjFgxaw8rlteyHkzcnW8gEgERUUgmNl7gR8BucATZrbBObcq4LJkCNUea+WR9aX8ct0+9lU3kZs+glveOY1rlhdqbUAkIFERCM65NcCaoOuQoVdS2cDPX9rLY28coKmlnSWTx/C5i2ayam6+tg2IBCwqAkHim3OOdSU13PdiCc9srSBlWBLvWTCB688uYl6B9hQSiRYKBBkybe0d/HFzGT99YTebD9YxNjWFT10wnX8+YzK56SOCLk9EulAgyKBrbm3nkfWl3Pu3EkprjlGck8q33juf9y0q0OmlRaKYAkEGTX1zK79ct4/7X9xDVUMLpxdm8eV3zeHC2Xm68LxIDFAgyIDVNrXy85f3cP+Le6hrbuO8Gbl84vypLJ8yVqcjEYkhCgQ5ZbVNrdz3Ygk/f2kv9cfbuHBOHresmMZpE7OCLk1EToECQfqtvrmV+1/cy89eLKG+uY1L5uVzy4rpzJmQEXRpIjIACgQJW3NrO79cu48fP7eLo02tXDQnj0+vnKEgEIkTCgTpU1t7B4+9cYC7n95JWV0z507P4fOrZqprSCTOKBCkR845/rq9gm//cRs7yhs4vTCLu69ayJlTs4MuTUSGgAJBurXlUB3f+MMW1pZUMyUnlXs+uIhVc/O115BIHFMgyEkq64/z/ae3s/q1UrJGDefrl83l2uWFDNcF6kXingJBAG87wYNr93H30zs41trO9WdN4dYLppM5enjQpYlIhCgQhFdKqvnq795me3k9507P4c7L5jI1Ny3oskQkwhQICaymsYW7ntjKY28coCBrFPd8cDGr5uZpO4FIglIgJCDnHI+8foBvPbmVhuY2PnH+VG5ZMZ1RKTrxnEgiUyAkmP3VTXxxzSZe2lXN0qIx3PXe+czISw+6LBGJAgqEBNHR4Xjg5b1898/bGZZkfPOKeVy7rFBnIRWRExQICaC0ponPPrKRV/fU8M6Zudz13vlMyNJ1i0XkZAqEOOacY/VrpXzzD1tIMuO7V57GlYsnaqOxiHRLgRCnahpbuP2xTTy9pZyzpmbz3Q8soEBrBSLSCwVCHHppVxX/+j8bONrUypffNZuPnj1F2wpEpE8KhDjS1t7Bfzyzkx8/t4vinFR+fv1S5k7IDLosEYkRCoQ4UVHXzC0Pvckre2q4askk7rxsro4rEJF+USDEgXUl1XzyN2/QeLydf//AAt6/eGLQJYlIDFIgxDDnvGMLvvnEViZnj+Y3N56hg8xE5JQpEGJUc2s7d6x5i/994yArZ+dx91ULSB+pM5OKyKlTIMSgyvrj3PjgejaUHuVfV87glhXTtBeRiAyYAiHGbCur44YH1lPdeJx7PriIi+eND7okEYkTCoQY8sKOSj7x6zcYnTKMRz5+FvMnapdSERk8CoQY8ds3D/K5RzYyPS+d+z+yhPGZOupYRAaXAiEG/PcLJdz15FbOKB7LvR9aQoY2HovIEFAgRDHnHN/583b+67ndXDo/n7uvWsiIZB1sJiJDQ4EQpZxzfP3xLTzw8l6uXV7INy6fxzDtSSQiQygp6AIAzOy7ZrbNzDaZ2Rozywq6piB1dDjuWLOZB17eyw3nTOGuKxQGIjL0oiIQgKeBec6504AdwBcDricwHR2O2x/bxEOv7udf3jmVL79rtq5fICIRERWB4Jx7yjnX5t9dByTkyXicc3z195t55PUD3HrBdD6/apbCQEQipt+BYGapZjaUWzY/Cvyxl9e/yczWm9n6ysrKISwjspxzfOMPW/nVuv3c/I6pfHrl9KBLEpEE02cgmFmSmV1rZk+YWQWwDThsZm/7ff9hLbnM7Bkz29zN3+Uh43wJaAN+3dN0nHP3OueWOOeW5ObmhvPSMeH7T+/g/pf28JGzirj94plaMxCRiAtnL6O/As/g9etvds51AJjZWOCdwLfNbI1z7le9TcQ5t7K34Wb2YeDdwAXOORdO8fHil2v38qNnd3HVkkl87T1zFAYiEohwAmGlc66164POuRrgMeAxMxvQkVJmdjFwO/AO51zTQKYVa/60uYyv/v5tVs4ex13vnacwEJHA9Nll1BkGZnaJmb1iZtvN7GEzO7PrOAPwn0A68LSZbTCzewY4vZjw2t4aPrX6TRZOyuJH1ywieVhUbOMXkQTVnwPTfgJ8ENgCLAa+a2Y/ds49NNAinHPTBjqNWLO/uombHlzPxKxR3PfhpbrcpYgErj+BUO6ce8m//YyZrQVeAQYcCImm4XgbNz64ng4H939kKWNTU4IuSUSkX7ud7jWzb5pZ59KrFagfgpriWkeH49OrN7CrsoGfXLeIopzUoEsSEQH6FwgOeB9QamYvAruA58Ld7VQ8//70dp7ZWs5X3jWbs6flBF2OiMgJYXcZOeeuATCzkcA8YIH/9zMzK3bOTRqaEuPHs9vK+fFfd3PVkkl8+KyioMsRETlJn4FgZhZ6XIBzrhlY7/91jqPdY/pw6OgxPvPwRmaPz+Drl8/V7qUiEnXCWZD/1cxuMbPC0AfNLMXMVpjZL4APDU158aG1vYNbHnqT1rYOfnLdIkYO1x5FIhJ9wukyuhjv/EIPmdkU4CgwEhgGPAXc7ZzbMFQFxoPvPbWd1/cd4YfXnM4UbUQWkSjVZyD4XUQ/AX7iH5GcAxxzzh0d4triwrqSan76fAnXLCvksgUTgi5HRKRH/bpimn9E8uEhqiXuNBxv4/OPbmRy9mi+8u7ZQZcjItKrsAPBzD7TzcO1wOvqMuret57cyoEjx3jk42cyOkVXKxWR6NafvYOWADcDBf7fTcD5wH+b2W2DX1pse35HJb95ZT83nlvMkqKxQZcjItKn/vxszQYWOecaAMzsa8CjwHnA68B3Br+82NRwvI3bH93EtHFpfObCGUGXIyISlv6sIRQCLSH3W4HJzrljwPFBrSrG/cfTOyivb+Y7V56mXUxFJGb0Zw3hN8A6M/sdYHgXs3nIzFLxzoAqwLayOn7+8l6uXlrIosIxQZcjIhK2/py64htm9iRwDl4g3Oyc6zxa+bqhKC7WdHQ4vrxmMxkjk7lt1cygyxER6Zf+nnKiDejw/w/0ojhx57E3DrB+3xG+eMlsxuiU1iISY8IOBDO7Ffg13oFp44BfmdktQ1VYrKlrbuXbf9zGosIsrlw8MehyRET6rT/bEG4AljvnGgHM7N+AtcCPhqKwWHPv8yVUN7bwwPXLSErSietEJPb0p8vIgPaQ++3+Ywmvoq6Z+17cw3sWTGD+xMygyxEROSX9WUP4OfCKma3BC4IrgPuHoqhY84O/7KS1vYPP6pgDEYlh/dnL6Ptm9hxwNl4gfFinrIA9VY2sfq2Ua5cV6nKYIhLTwrlATj3e5TNPPBQyzDnnMoaisFjxvae2MyI5iVsumBZ0KSIiAxLO6a/TI1FILNpyqI4nNh3mUyumMS59ZNDliIgMiC59OQA/fWE3qSnDuOGc4qBLEREZMAXCKSqtaeIPmw5zzbJCMkcPD7ocEZEBUyCcovte3EOSwQ3nTgm6FBGRQaFAOAU1jS2sfm0/ly8sYHzmqKDLEREZFAqEU/CLl/fS3NrBze/QtgMRiR8KhH5qamnjwbV7WTk7j2njtAOWiMQPBUI/Pb7xEEeaWrnpPK0diEh8USD00+rXSpk2Lo2lRbr4jYjEFwVCP2wvq+fN/Ue5eukkzHRePxGJLwqEflj92n5ShiXxvkW63oGIxB8FQpiaW9tZ8+ZBLpqbx1hdDU1E4lBUBIKZfcPMNpnZBjN7yswmBF1TV39+u4yjTa1cvbQw6FJERIZEVAQC8F3n3GnOuYXAH4CvBlzPP1j9aimTxo7irKnZQZciIjIkoiIQnHN1IXdTOfl024HbW9XI2pJqrl5aqMtjikjc6s8V04aUmd0FfAioBd7Zy3g3ATcBFBZGpvvmibcOA/C+RQUReT0RkSBEbA3BzJ4xs83d/F0O4Jz7knNuEvBr4JM9Tcc5d69zbolzbklubm5Eav/T5jJOL8zSeYtEJK5FbA3BObcyzFF/AzwBfG0IywnbgSNNvHWwli9eMivoUkREhlRUbEMws+khdy8DtgVVS1d/2lwGwKq5+QFXIiIytKJlG8K3zWwm0AHsA24OuJ4T/vx2GbPy0ynKSQ26FBGRIRUVgeCce3/QNXSnor6Z9fuOcOsF0/seWUQkxkVFl1G0enpLOc7BxfPUXSQi8U+B0Is/bS6jKHs0M/N03QMRiX8KhB7UNrWydnc1F88brzObikhCUCD04K/bK2jrcKyamxd0KSIiEaFA6MHa3dVkjEzmtIlZQZciIhIRCoQevLKnmmVTshmmcxeJSIJQIHTjcO0x9lY3cUbx2KBLERGJGAVCN14pqQHgjGKd6lpEEocCoRvrSqpJH5nM7PEZQZciIhIxCoRurCupZvmUsdp+ICIJRYHQRVlts7/9QN1FIpJYFAhdvLKnGtD2AxFJPAqELrT9QEQSlQKhi3UlNSwr0vYDEUk8CoQQ5XXN7KlqVHeRiCQkBUKIdSXafiAiiUuBEGLzwVpSkpOYPV6nuxaRxKNACLGtrJ4ZeWkkD9PbIiKJR0u+ENvK6pmZp72LRCQxKRB8NY0tVNYfZ1a+uotEJDEpEHzbyuoAmKXtByKSoBQIvu1l9QDM1BqCiCQoBYJv2+F6xqamkJs2IuhSREQCoUDwbSuvZ2ZeOmY6QllEEpMCAejocOwoq9f2AxFJaAoEYH9NE8da27WHkYgkNAUC3vEHADPzdQyCiCQuBQLeHkZmMCMvLehSREQCo0DAOwZh8tjRjE5JDroUEZHAKBDw1hB0/IGIJLqED4Tm1nb2VjcyS9sPRCTBJXwg7CxvoMOhPYxEJOElfCBs9c9hpC4jEUl0CR8IJZWNpAxLYnJ2atCliIgEKqoCwcw+Z2bOzHIi9Zrldc2MyxjBsCSdskJEElvUBIKZTQIuBPZH8nXLapvJyxgZyZcUEYlKURMIwN3AbYCL5IuW1zeTr0AQEYmOQDCzy4CDzrmNYYx7k5mtN7P1lZWVA37tcq0hiIgAELFDc83sGSC/m0FfAu4ALgpnOs65e4F7AZYsWTKgtYmG4200trSTl6FrIIiIRCwQnHMru3vczOYDU4CN/rUIJgJvmNky51zZUNZUVtsMQH6m1hBERAI/eY9z7i1gXOd9M9sLLHHOVQ31a5fXeYEwLl2BICISFdsQgtIZCFpDEBGJgjWErpxzRZF6rTI/ELQNQUQkwdcQKuqOkz4yWae9FhEhwQOhrFbHIIiIdErsQKjTMQgiIp0SOhAqFAgiIickbCB0dDgq6o9rg7KIiC9hA6Gq8ThtHU67nIqI+BI2ECrqjgOoy0hExJewgdB52goFgoiIJ2EDobzeP0pZgSAiAiRyINQ2k2SQk5YSdCkiIlEhYQOhrK6ZnLQRJA9L2LdAROQkCbs0LK87rj2MRERCJHAgNOu01yIiIRI6EPIzdVCaiEinhAyE5tZ2jjS1kqc1BBGRExIyEE4clKZtCCIiJyRkIOgYBBGRf5SQgaCjlEVE/lFCBsKJaykrEERETkjYQBiRnETGKF06U0SkU0IGwtTcNK5YWICZBV2KiEjUSMifyFcvK+TqZYVBlyEiElUScg1BRET+kQJBREQABYKIiPgUCCIiAigQRETEp0AQERFAgSAiIj4FgoiIAGDOuaBrOGVmVgns68dTcoCqISonmqndiSVR2w2J2/b+tnuycy6364MxHQj9ZWbrnXNLgq4j0tTuxJKo7YbEbftgtVtdRiIiAigQRETEl2iBcG/QBQRE7U4sidpuSNy2D0q7E2obgoiI9CzR1hBERKQHCgQREQHiNBDM7GIz225mu8zsC90MNzP7oT98k5ktCqLOwRZGu6/z27vJzF42swVB1DnY+mp3yHhLzazdzK6MZH1DJZx2m9n5ZrbBzN42s+cjXeNQCGM+zzSzx81so9/u64Ooc7CZ2f1mVmFmm3sYPvDlmnMurv6AYcBuoBhIATYCc7qMcynwR8CAM4BXgq47Qu0+Cxjj374kUdodMt6zwJPAlUHXHaHPOwvYAhT698cFXXeE2n0H8G/+7VygBkgJuvZBaPt5wCJgcw/DB7xci8c1hGXALudciXOuBVgNXN5lnMuBB51nHZBlZuMjXegg67PdzrmXnXNH/LvrgIkRrnEohPN5A9wCPAZURLK4IRROu68F/tc5tx/AORcPbQ+n3Q5IN++i6Wl4gdAW2TIHn3PuBby29GTAy7V4DIQCoDTk/gH/sf6OE2v626Yb8H5NxLo+221mBcB7gXsiWNdQC+fzngGMMbPnzOx1M/tQxKobOuG0+z+B2cAh4C3gVudcR2TKC9SAl2vJg1pOdLBuHuu6b20448SasNtkZu/EC4RzhrSiyAin3f8B3O6ca/d+NMaFcNqdDCwGLgBGAWvNbJ1zbsdQFzeEwmn3KmADsAKYCjxtZn9zztUNcW1BG/ByLR4D4QAwKeT+RLxfCv0dJ9aE1SYzOw34GXCJc646QrUNpXDavQRY7YdBDnCpmbU5534bkQqHRrjzeZVzrhFoNLMXgAVALAdCOO2+Hvi28zrWd5nZHmAW8GpkSgzMgJdr8dhl9Bow3cymmFkKcDXw+y7j/B74kL9V/gyg1jl3ONKFDrI+221mhcD/Av8c478SQ/XZbufcFOdckXOuCHgU+ESMhwGEN5//DjjXzJLNbDSwHNga4ToHWzjt3o+3VoSZ5QEzgZKIVhmMAS/X4m4NwTnXZmafBP6Mt0fC/c65t83sZn/4PXh7mlwK7AKa8H5RxLQw2/1VIBv4if9ruc3F+Jkhw2x33Amn3c65rWb2J2AT0AH8zDnX7S6LsSLMz/sbwANm9hZeN8rtzrmYPyW2mT0EnA/kmNkB4GvAcBi85ZpOXSEiIkB8dhmJiMgpUCCIiAigQBAREZ8CQUREAAWCiIj4FAgiIgIoEERExBd3B6aJBMnMVuMdDFUE5OMdFf1EoEWJhElrCCKDawFQ4pxbDlyHdzSpSEzQkcoig8TMRuGdR2eSc67ZzMbiXaRkesCliYRFawgig2cesNM51+zfX4R3RS+RmKBtCCKDZwFQaGYj8U689nXgtmBLEgmfAkFk8CwAfg08B2QA33LOvRRoRSL9oG0IIoPEvwDNjc657UHXInIqFAgig8TMDuJtUE6E6/dKHFIgiIgIoL2MRETEp0AQERFAgSAiIj4FgoiIAAoEERHxKRBERARQIIiIiO//A53CrDyv04WjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_ln()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26fe6f-d5ad-4a55-8bfc-fd373d9f62bc",
   "metadata": {},
   "source": [
    "Let's keep working on the last expression above, which seeks the weight vector that maximizes the sum of the log of the probability of all the training instances.  The first step below is to rewrite $\\ln \\left( p_{\\bf w}({\\bf x_i}, y_i) \\right)$ as $\\ln \\left( p_{\\bf w}(y_i|{\\bf x_i}) p_{\\bf w}({\\bf x_i})\\right)$ using the chain rule.  That can be expanded into $\\ln(p_{\\bf w}(y_i|{\\bf x_i})) + \\ln(p_{\\bf w}({\\bf x_i}))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be55929-3815-49da-89d9-add4e6db76fc",
   "metadata": {},
   "source": [
    "The second term is the prior probability of ${\\bf x_i}$, or how probable it is to observe that particular attribute vector.  It could be the case that all attribute vectors are equally likely or, more realistically, it could be the case that some are more probable than others.  Regardless, those probabilities are independent of the weight vector.  Nature doesn't care what Logistic Regression does, she just offers up $(\\bf x, y)$ pairs for it to learn from.  The $\\ln(p_{\\bf w}({\\bf x_i}))$ terms become constants that can be ignored inside the ${\\tt argmax}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec483be2-f97b-4cab-8481-e79d51dc71e5",
   "metadata": {},
   "source": [
    "That just leaves $\\ln(p_{\\bf w}(y_i|{\\bf x_i}))$, which is the log of the probability that the model assigns to the *correct* class label for the $i^{th}$ instance given the current weight vector.  If $y_i = 1$ that probability is $p_{\\bf w}^1({\\bf x_i})$, and if $y_i = 0$ that probability is $p_{\\bf w}^0({\\bf x_i})$.  That's written below in the last line as an if-then rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb119fee-c80f-4ed8-8ba5-f02128a59b75",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\underset{{\\bf w}}{\\arg\\max} \\ \\sum_{i} \\ln \\left( p_{\\bf w}({\\bf x_i}, y_i) \\right) & = & \\underset{{\\bf w}}{\\arg\\max} \\ \\sum_{i} \\ln \\left( p_{\\bf w}(y_i|{\\bf x_i}) p_{\\bf w}({\\bf x_i})\\right) \n",
    "\\tag*{Chain rule} \\\\\n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\sum_{i} \\ln \\left( p_{\\bf w}(y_i|{\\bf x_i}) \\right) \n",
    "\\tag*{${\\bf x_i}$ independent of ${\\bf w}$}\n",
    "\\\\ \n",
    "& = & \\underset{{\\bf w}}{\\arg\\max} \\sum_{i} \\begin{cases}\n",
    "    \\ln(p_{\\bf w}^1({\\bf x_i}))& \\text{if } y_i = 1\\\\\n",
    "    \\ln(p_{\\bf w}^0({\\bf x_i}))& \\text{if } y_i = 0\\\\\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55ad89-f345-4ec3-a075-9988f435bfd6",
   "metadata": {},
   "source": [
    "The if-then rule in the line above is problematic for the purposes of taking partial derivatives.  A neat way around that is to use the cross-entropy form to turn the rule into an equation that easily admits gradient descent.  That form is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74848872-eafd-470e-b508-712423d3c6c0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ln(p_{\\bf w}(y_i|{\\bf x_i})) = y_i \\ln(p_{\\bf w}^1({\\bf x_i})) + (1 - y_i)\n",
    "\\ln(1 - p_{\\bf w}^1({\\bf x_i}))\n",
    "$$        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad13cdf-b43d-4452-97b2-50840da50eb9",
   "metadata": {},
   "source": [
    "If $y_i = 1$ the expression above simplifies as follows:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\ln(p_{\\bf w}(y_i = 1|{\\bf x_i})) & = & 1 * \\ln(p_{\\bf w}^1({\\bf x_i})) + (1 - 1) *\n",
    "\\ln(1 - p_{\\bf w}^1({\\bf x_i})) \\\\\n",
    "& = & \\ln(p_{\\bf w}^1({\\bf x_i})) + 0 * \\ln(1 - p_{\\bf w}^1({\\bf x_i})) \\\\\n",
    "& = & \\ln(p_{\\bf w}^1({\\bf x_i}))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Similarly, when $y_i = 0$ the expression simplifies to $\\ln(1 - p_{\\bf w}^1({\\bf x_i}))$.  The cross entropy form gives us an equation that is differentiable and behaves exactly like the rule-based form!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab6f7-32c3-4ff5-be3c-f0359a419f90",
   "metadata": {},
   "source": [
    "We've finally arrived at the function that Logisic Regression will optimize:\n",
    "\n",
    "$$\n",
    "L(D; {\\bf w}) = \\sum_i \\left[y_i \\ln(p_{\\bf w}^1({\\bf x_i})) + (1 - y_i)\n",
    "\\ln(1 - p_{\\bf w}^1({\\bf x_i})) \\right]\n",
    "$$        \n",
    "\n",
    "Because the goal is to maximize the log likelihood we'll use gradient ascent, but it's easy to use gradient descent to minimize the negative log likelihood, i.e., $-L(D; {\\bf w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e7ca5-ed50-4e30-b2f7-19eb398764e2",
   "metadata": {},
   "source": [
    "### Maximizing the Likelihood with Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e7be5-ae0a-4378-ab98-1486ad209287",
   "metadata": {},
   "source": [
    "All that's left is to compute the gradient of $L(D; {\\bf w})$, i.e., the partial derivatives with respect to the individual weights.  Recall that the generic weight update rule for gradient descent given learning rate $\\alpha$ and loss function $L$ is:\n",
    "\n",
    "$$\n",
    "{\\bf w} = {\\bf w} - \\alpha \\nabla L\n",
    "$$\n",
    "\n",
    "Iterating between computing $\\nabla L$ and updating the weights minimizes $L$.  All that's required to maximimize $L$ is to make the update additive:\n",
    "\n",
    "$$\n",
    "{\\bf w} = {\\bf w} + \\alpha \\nabla L\n",
    "$$\n",
    "\n",
    "Now we're walking uphill rather than downhill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18df1b-14c3-4da0-ba1c-9a142dea2ea7",
   "metadata": {},
   "source": [
    "We'll need to remember two facts from differential calculus.  The first is about the derivative of the log function:\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln(x)}{dx} = \\frac{1}{x} dx\n",
    "$$\n",
    "\n",
    "The second is about the derivative of exponentials:\n",
    "\n",
    "$$\n",
    "\\frac{d e^x}{dx} = e^x dx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab3596-618a-407e-ae28-09e7f10b58ca",
   "metadata": {},
   "source": [
    "Let's start taking the derivative of the log likelihood with respect to an arbitrary weight $w_j$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial w_j} L(D; {\\bf w}) & = & \\frac{\\partial}{\\partial w_j} \\sum_i \\left[ y_i \\ln(p_{\\bf w}^1({\\bf x_i})) + (1 - y_i)\n",
    "\\ln(1 - p_{\\bf w}^1({\\bf x_i})) \\right] \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i}{p_{\\bf w}^1({\\bf x_i})} \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) + \\frac{(1 - y_i)}{(1 - p_{\\bf w}^1({\\bf x_i}))} \n",
    "\\frac{\\partial}{\\partial w_j} (1 - p_{\\bf w}^1({\\bf x_i})) \\right] \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i}{p_{\\bf w}^1({\\bf x_i})} \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) - \\frac{(1 - y_i)}{(1 - p_{\\bf w}^1({\\bf x_i}))} \n",
    "\\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) \\right] \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i}{p_{\\bf w}^1({\\bf x_i})} - \\frac{(1 - y_i)}{(1 - p_{\\bf w}^1({\\bf x_i}))} \n",
    "\\right] \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i (1 - p_{\\bf w}^1({\\bf x_i})) - (1 - y_i) p_{\\bf w}^1({\\bf x_i})}{p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})} \n",
    "\\right] \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i - p_{\\bf w}^1({\\bf x_i})}{p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})} \n",
    "\\right] \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) \\\\[0.25cm]\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a399cb-ae75-40b4-8977-f601c91e4200",
   "metadata": {},
   "source": [
    "Now we're ready to compute the partial derivative of $p_{\\bf w}^1({\\bf x_i})$ with respect to weight $w_j$.  Below, $x_{i,j}$ denotes the $j^{th}$ element of the $i^{th}$ instance's attribute vector.  That element is multiplied by $w_j$ when computing ${\\bf w} \\cdot {\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17132f-3ea4-4bad-98fa-ac2082a839fd",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) & = &\n",
    "\\frac{\\partial}{\\partial w_j} \\frac{1}{1 + e^{-{\\bf w} \\cdot {\\bf x_i}}} \\\\[0.2cm]\n",
    "& = & - \\frac{1}{({1 + e^{-{\\bf w} \\cdot {\\bf x_i}}})^2} \\frac{\\partial}{\\partial w_j}\n",
    "e^{-{\\bf w} \\cdot {\\bf x}} \\\\[0.2cm]\n",
    "& = & - \\frac{1}{({1 + e^{-{\\bf w} \\cdot {\\bf x_i}}})^2} \n",
    "e^{-{\\bf w} \\cdot {\\bf x_i}} \\frac{\\partial}{\\partial w_j} (-{\\bf w} \\cdot {\\bf x_i}) \\\\[0.2cm]\n",
    "& = & \\frac{e^{-{\\bf w} \\cdot {\\bf x_i}}}{({1 + e^{-{\\bf w} \\cdot {\\bf x_i}}})^2} \\ x_{i,j} \\\\[0.2cm]\n",
    "& = & \\frac{1}{({1 + e^{-{\\bf w} \\cdot {\\bf x_i}}})}\n",
    "\\frac{e^{-{\\bf w} \\cdot {\\bf x_i}}}{({1 + e^{-{\\bf w} \\cdot {\\bf x_i}}})}\n",
    "\\ x_{i,j} \\\\[0.2cm]\n",
    "& = & p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})) \\ x_{i,j} \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe252b4f-5eec-45b3-8972-af9dd3e7905d",
   "metadata": {},
   "source": [
    "The final step is to replace the partial derivative of $p_{\\bf w}^1({\\bf x_i})$ with respect to $w_j$ in the expression below and simplify:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955fde5-0db5-47c5-8971-577da5bed26b",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial w_j} L(D; {\\bf w}) & = & \n",
    "\\sum_i \\left[ \\frac{y_i - p_{\\bf w}^1({\\bf x_i})}{p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})} \n",
    "\\right] \\frac{\\partial}{\\partial w_j} p_{\\bf w}^1({\\bf x_i}) \\\\[0.25cm]\n",
    "& = & \\sum_i \\left[ \\frac{y_i - p_{\\bf w}^1({\\bf x_i})}{p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})} \\right] p_{\\bf w}^1({\\bf x_i}) (1 - p_{\\bf w}^1({\\bf x_i})) \\ x_{i,j} \\\\[0.25cm]\n",
    "& = & \\sum_i (y_i - p_{\\bf w}^1({\\bf x_i})) x_{i,j}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5a086-7315-4fd0-8dbc-0fe48b673770",
   "metadata": {},
   "source": [
    "That was a lot of work to wind up with such a simple expression! But now that we've got it let's see why it makes intuitive sense by considering how it is used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a004868-8238-4d42-b51c-76c725134469",
   "metadata": {},
   "source": [
    "### Intuition Behind the Update Rule for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507d8f0-9472-4c62-a444-3677cbb4bc33",
   "metadata": {},
   "source": [
    "Recall that gradient ascent for Logistic Regression involves iterating between computing the gradient of the log likelihood and taking a step in the direction of the gradient.\n",
    "\n",
    "$$\n",
    "{\\bf w} = {\\bf w} + \\alpha \\nabla L(D; {\\bf w})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda009d-400a-4c3e-a229-d2ec1ecd58e9",
   "metadata": {},
   "source": [
    "Let's focus on a single instance $({\\bf x_i}, y_i)$ and a single weight $w_j$, for which the update is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef8b69-dba3-415d-aef4-44b8aafc3958",
   "metadata": {},
   "source": [
    "$$\n",
    "w_j = w_j + \\alpha (y_i - p_{\\bf w}^1({\\bf x_i})) x_{i,j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fdeeb-cce1-41a2-9e1f-2fd5cbc93adb",
   "metadata": {},
   "source": [
    "The new value of $w_j$ is the old value plus (because we're doing gradient ascent) a fraction $\\alpha$ of the $j^{th}$ term in the gradient of the log likelihood.  Let's consider a couple of cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2481bb1-34ea-40ee-925a-2ada2cb23d4e",
   "metadata": {},
   "source": [
    "**Case 1:** $y_i = 1$ and $x_{i,j} > 0$\n",
    "\n",
    "If $y_i = 1$ and $x_{i,j} > 0$, we want $w_j$ to increase.  Why?  Because we want ${\\bf w} \\cdot {\\bf x_i}$ to be as large as possible so we move to the right on the plot of the sigmoid above and $p_{\\bf w}^1({\\bf x_i})$ gets closer to 1, assigning a higher probability than before the update to the correct class label.\n",
    "\n",
    "If $y_i = 1 $ then $y_i - p_{\\bf w}^1({\\bf x_i}) > 0$ (note that $p_1$ only approaches 1 asymptotically).  Also, in this case $x_{i,j} > 0$ so $(y_i - p_{\\bf w}^1({\\bf x_i})) x_{i,j} > 0$.  That means that after the update $w_j$ will increase, which is precisely what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e18326-ae1f-4c3d-87bc-8348528ab618",
   "metadata": {},
   "source": [
    "**Case 2:** $y_i = 1$ and $x_{i,j} < 0$\n",
    "\n",
    "What needs to happen to $w_j$ in this case?  We still want ${\\bf w} \\cdot {\\bf x_i}$ to be a big positive number because $y_i = 1$.  The way to make that happen is to multiply $x_{i,j}$, which is negative, by a weight that is also negative.  The smaller (more negative) the value of $w_j$ the larger the value of $w_j x_{i,j}$ and the closer $p_{\\bf w}^1({\\bf x_i})$ is to 1.\n",
    "\n",
    "Because $y_i - p_{\\bf w}^1({\\bf x_i}) > 0$ and  $x_{i,j} < 0$ it is the case that $(y_i - p_{\\bf w}^1({\\bf x_i})) x_{i,j} < 0$.  That means that after the update $w_j$ will decrease, which is precisely what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653bd078-86d6-43e7-9a83-2c8e64908b2a",
   "metadata": {},
   "source": [
    "A similar analysis when $y_i = 0$ leads to the same conclusion.  In that case $y_i - p_{\\bf w}^1({\\bf x_i}) < 0$, but regardless of the sign of $x_{i,j}$ the update moves the weight in the right direction, making $p_{\\bf w}^1({\\bf x_i}))$ closer to 0 and thus $p_{\\bf w}^0({\\bf x_i}))$ closer to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccaa6c2-46cd-4512-874d-ab2160049c4d",
   "metadata": {},
   "source": [
    "Now that we've got the gradient in hand, we can turn to the learning algorithm implementation, which is the topic of the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f68add-3671-456d-81b4-81ebe0631d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
